{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1- Explain the following\n",
        "\n",
        "Artificial Intelligence\n",
        "\n",
        " Machine Learning,\n",
        "\n",
        "Deep Learning\n",
        "\n",
        "\n",
        "\n",
        "**Artificial intelligence (AI**) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
        "\n",
        "The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal. A subset of artificial intelligence is machine learning (ML), which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans. Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.\n",
        "\n",
        "When most people hear the term artificial intelligence, the first thing they usually think of is robots. That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth. But nothing could be further from the truth.\n",
        "\n",
        "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity. Researchers and developers in the field are making surprisingly rapid strides in mimicking activities such as learning, reasoning, and perception, to the extent that these can be concretely defined. Some believe that innovators may soon be able to develop systems that exceed the capacity of humans to learn or reason out any subject. But others remain skeptical because all cognitive activity is laced with value judgments that are subject to human experience.\n",
        "\n",
        "\n",
        "\n",
        "**Machine learning** is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.\n",
        "\n",
        "The goal of AI is to create computer models that exhibit “intelligent behaviors” like humans, according to Boris Katz, a principal research scientist and head of the InfoLab Group at CSAIL. This means machines that can recognize a visual scene, understand a text written in natural language, or perform an action in the physical world.\n",
        "\n",
        "Machine learning is one way to use AI. It was defined in the 1950s by AI pioneer Arthur Samuel as “the field of study that gives computers the ability to learn without explicitly being programmed.”\n",
        "\n",
        "The definition holds true, according toMikey Shulman, a lecturer at MIT Sloan and head of machine learning at Kensho, which specializes in artificial intelligence for the finance and U.S. intelligence communities. He compared the traditional way of programming computers, or “software 1.0,” to baking, where a recipe calls for precise amounts of ingredients and tells the baker to mix for an exact amount of time. Traditional programming similarly requires creating detailed instructions for the computer to follow.\n",
        "\n",
        "But in some cases, writing a program for the machine to follow is time-consuming or impossible, such as training a computer to recognize pictures of different people. While humans can do this task easily, it’s difficult to tell a computer how to do it. Machine learning takes the approach of letting computers learn to program themselves through experience. \n",
        "\n",
        "Machine learning starts with data — numbers, photos, or text, like bank transactions, pictures of people or even bakery items, repair records, time series data from sensors, or sales reports. The data is gathered and prepared to be used as training data, or the information the machine learning model will be trained on. The more data, the better the program.\n",
        "\n",
        "\n",
        "There are three subcategories of machine learning:\n",
        "\n",
        "Supervised machine learning models are trained with labeled data sets, which allow the models to learn and grow more accurate over time. For example, an algorithm would be trained with pictures of dogs and other things, all labeled by humans, and the machine would learn ways to identify pictures of dogs on its own. Supervised machine learning is the most common type used today.\n",
        "\n",
        "In unsupervised machine learning, a program looks for patterns in unlabeled data. Unsupervised machine learning can find patterns or trends that people aren’t explicitly looking for. For example, an unsupervised machine learning program could look through online sales data and identify different types of clients making purchases.\n",
        "\n",
        "Reinforcement machine learning trains machines through trial and error to take the best action by establishing a reward system. Reinforcement learning can train models to play games or train autonomous vehicles to drive by telling the machine when it made the right decisions, which helps it learn over time what actions it should take.\n",
        "\n",
        "Deep Learning\n",
        "\n",
        "What is Deep Learning? Deep learning is a branch of machine learning which is completely based on artificial neural networks, as neural network is going to mimic the human brain so deep learning is also a kind of mimic of human brain. In deep learning, we don’t need to explicitly program everything. The concept of deep learning is not new. It has been around for a couple of years now. It’s on hype nowadays because earlier we did not have that much processing power and a lot of data. As in the last 20 years, the processing power increases exponentially, deep learning and machine learning came in the picture. A formal definition of deep learning is- neurons\n",
        "\n",
        "Deep Learning is a subset of Machine Learning that is based on artificial neural networks (ANNs) with multiple layers, also known as deep neural networks (DNNs). These neural networks are inspired by the structure and function of the human brain, and they are designed to learn from large amounts of data in an unsupervised or semi-supervised manner.\n",
        "\n",
        "Deep Learning models are able to automatically learn features from the data, which makes them well-suited for tasks such as image recognition, speech recognition, and natural language processing. The most widely used architectures in deep learning are feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\n",
        "\n",
        "Feedforward neural networks (FNNs) are the simplest type of ANN, with a linear flow of information through the network. FNNs have been widely used for tasks such as image classification, speech recognition, and natural language processing.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e5gALKgzfaPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. what is supervised machine learning list of some examples of supervised learning \n",
        "\n",
        "Supervised Machine Learning\n",
        "\n",
        "Supervised learning is the types of machine learning in which machines are trained using well \"labelled\" training data, and on basis of that data, machines predict the output. The labelled data means some input data is already tagged with the correct output.\n",
        "\n",
        "In supervised learning, the training data provided to the machines work as the supervisor that teaches the machines to predict the output correctly. It applies the same concept as a student learns in the supervision of the teacher.\n",
        "\n",
        "Supervised learning is a process of providing input data as well as correct output data to the machine learning model. The aim of a supervised learning algorithm is to find a mapping function to map the input variable(x) with the output variable(y).\n",
        "\n",
        "In the real-world, supervised learning can be used for Risk Assessment, Image classification, Fraud Detection, spam filtering, etc.\n",
        "\n",
        "Supervised learning can be further divided into two types of problems:\n",
        "\n",
        "Supervised Machine learning\n",
        "1. Regression\n",
        "\n",
        "Regression algorithms are used if there is a relationship between the input variable and the output variable. It is used for the prediction of continuous variables, such as Weather forecasting, Market Trends, etc. Below are some popular Regression algorithms which come under supervised learning:\n",
        "\n",
        "Linear Regression\n",
        "Regression Trees\n",
        "Non-Linear Regression\n",
        "Bayesian Linear Regression\n",
        "Polynomial Regression\n",
        "2. Classification\n",
        "\n",
        "Classification algorithms are used when the output variable is categorical, which means there are two classes such as Yes-No, Male-Female, True-false, etc.\n",
        "\n",
        "Spam Filtering,\n",
        "\n",
        "Random Forest\n",
        "Decision Trees\n",
        "Logistic Regression\n",
        "Support vector Machines\n",
        "\n",
        "Image and object recognition\n",
        "Supervised machine learning is used to locate, categorise and isolate objects from images or videos, which is useful when applied to different imagery analysis and vision techniques. The primary goal of image or object recognition is to identify the image accurately.\n",
        "\n",
        "Example: We use the ML to recognise the image precisely as if it is the image of the plane or a car or if the image is of a cat or a dog.\n",
        "\n",
        "Predictive analytics\n",
        "Supervised machine learning models are widely used in building predictive analytics systems, which provide in-depth insights into different business data points. This enables the organisations to predict certain results using the output given by the system. It also helps business leaders to make decisions for the betterment of the company.\n",
        "\n",
        "Example 1: We may use supervised learning to predict house prices. Data having details about the size of the house, price, the number of rooms in the house, garden and other features are needed. We need data about various parameters of the house for thousands of houses and it is then used to train the data. This trained supervised machine learning model can now be used to predict the price of a house.\n",
        "\n",
        "Example 2: Spam detection is another area where most organisations use supervised machine learning algorithms. Data scientists classify different parameters to differentiate between official mail or spam mail. They use these algorithms to train the database such that the trained database recognise patterns in new data and classify them into spam and non-spam communication efficiently.\n",
        "\n",
        "Sentiment analysis\n",
        "Organisations can use supervised machine learning algorithms to predict customer sentiments. They use the algorithms to extract and categorise important information from large data sets like emotions, intent and context with little human interference. This model of supervised learning is also used to predict the sentiments of the text. This information is highly useful to gain insights about customer needs and help to improve brand-customer engagement efforts.\n",
        "\n",
        "Example: Some organisations, especially e-commerce stores, often try to identify the sentiments of their customer via product reviews posted on their applications or websites.\n"
      ],
      "metadata": {
        "id": "QX2dGtdghujO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.what is unsupervised machine learning list of some examples of unsupervised learning\n",
        "\n",
        "Unsupervised machine learning is the process of inferring underlying hidden patterns from historical data. Within such an approach, a machine learning model tries to find any similarities, differences, patterns, and structure in data by itself. No prior human intervention is needed.\n",
        "\n",
        "Let’s get back to our example of a child’s experiential learning.\n",
        "\n",
        "Picture a toddler. The child knows what the family cat looks like (provided they have one) but has no idea that there are a lot of other cats in the world that are all different. The thing is, if the kid sees another cat, he or she will still be able to recognize it as a cat through a set of features such as two ears, four legs, a tail, fur, whiskers, etc.\n",
        "\n",
        "In machine learning, this kind of prediction is called unsupervised learning. But when parents tell the child that the new animal is a cat – drumroll – that’s considered supervised learning.\n",
        "\n",
        "Unsupervised learning finds a myriad of real-life applications, including:\n",
        "\n",
        "data exploration,\n",
        "customer segmentation,\n",
        "recommender systems,\n",
        "target marketing campaigns, and\n",
        "data preparation and visualization, etc.\n",
        "\n",
        "Organize computing clusters − The geographic areas of servers is determined on the basis of clustering of web requests received from a specific area of the world. The local server will include only the data frequently created by people of that region.\n",
        "\n",
        "Social network analysis − Social network analysis is conducted to make clusters of friends depends on the frequency of connection between them. Such analysis reveals the links between the users of some social networking website.\n",
        "\n",
        "Market segmentation − Sales organizations can cluster or group their users into multiple segments on the basis of their prior billed items. For instance, a big superstore can required to send an SMS about grocery elements specifically to its users of grocery rather than sending that SMS to all its users.\n",
        "\n",
        "It is not only is it cheaper but also superior; after all it can be an irrelevant irritant to those who only buy clothing from the store. The combining of users into multiple segments based on their buy history will provide the store to focus the correct users for increasing sales and enhancing its profits.\n",
        "\n",
        "Astronomical data analysis − Astronomers need high telescopes to study galaxies and stars. The design in light or combining of lights received from multiple parts of the sky help to recognize multiple galaxies, planets, and satellites."
      ],
      "metadata": {
        "id": "-H9PQ-OBrpVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 what is difference between AI ,ML, DL and DS\n",
        "\n",
        "What is Data Science?\n",
        "Data Science is a systematic and scientific approach that defines processes involved to extract knowledge and insights from structured or unstructured data.\n",
        "\n",
        "Structured and Unstructured data\n",
        "In this digital era, the complexity of big data has presented both an opportunity and challenges to the organizations. It is overwhelming for most organizations to process and analyze data, which is being generated at an exponential rate. Statistically, 80% accounts of unstructured data being produced at around 45% Exabytes per year whereas, structured data constitutes 20% being created at around 25% Exabytes per year.\n",
        "\n",
        "\n",
        "Growth of Structured vs Unstructured Data\n",
        "Structured data is highly organized and formatted that can be easily analyzed and processed through relational databases. Structured data is most often categorized as quantitative data. Examples of structured data include gender, addresses, credit card numbers, stock information, geolocation, and more.\n",
        "\n",
        "Unstructured data has no pre-defined format, making it much more difficult to collect, process, and analyze using conventional tools and methods. It is most often categorized as qualitative data that is conceptually stored in Data Lakes (NoSQL databases). Examples of unstructured data include text, video, audio, social media, satellite imagery and the list goes on.\n",
        "\n",
        "Systematic and Scientific approach\n",
        "A systematic and scientific approach is process where theories and methodologies are practiced on complex problems to explore observations and predict outcomes.\n",
        "\n",
        "Theories are set of assumptions, principles and relationships for dataset to explain a certain phenomenon whereas, methods include observations, interviews & surveys, research and experiments. This model suggests that the outcome from testing the hypothesis or theory helps in deciding the appropriate actions to be taken.\n",
        "\n",
        "\n",
        "Systematic and scientific approach steps\n",
        "Extract Knowledge and Insights\n",
        "While the raw data is being processed using the systematic and scientific methodologies, it yields out information. When enough information is experienced (study and research), it becomes knowledge, and it is directly proportional to the quantity of processed data.\n",
        "\n",
        "On the flipside, insights are acquired when knowledge is aligned and observed with the problem statement in action. Moreover, iteratively, the experienced insights further evolve our knowledge.\n",
        "\n",
        "This churning of data producing insights is what desired and practiced by organizations to help them make effective decisions. Additionally, these insights (on data) are deduced by applying descriptive and inferential techniques.\n",
        "\n",
        "Briefly, Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations, tabulations, or simple visualizations techniques. Whereas, Inferential statistics makes inferences and predictions on a sample of data taken from the population in question\n",
        "\n",
        "What is Machine Learning?\n",
        "Machine Learning is process that starts with learning from data, finding patterns using algorithm and statistic techniques that help take actions with minimal human intervention.\n",
        "\n",
        "Learning from Data\n",
        "Learning (from data) is the process of gaining new, or modifying existing knowledge through research, study or experience. It is an iterative process of feeling/experiencing -> watching -> thinking -> doing.\n",
        "\n",
        "Applying this basic learning technique to data can help extract variety of insights that assist us in measuring data quality, giving scope and context to dataset, analyzing and modelling data, hypothesizing problem statement, reporting success metrics.\n",
        "\n",
        "\n",
        "It is important to note that David Kolb’s learning model could be an ideal model in the applications of advanced machine learning including Artificial Intelligence (AI)\n",
        "\n",
        "Finding Patterns\n",
        "Finding patterns one of the essential step in Machine Learning. Unless any definite patterns are observed from the data, it is very difficult to frame such a model for prediction. It is an ability to identify the characteristics of data that yield information about a given dataset.\n",
        "\n",
        "Patterns occur at regular intervals and repeats itself in a predictable manner. Once they are detected in a dataset, it becomes easier to classify and segregate the data for analysis.\n",
        "\n",
        "\n",
        "Steps involved in finding patterns\n",
        "Algorithm and Statistic Techniques\n",
        "Algorithm is a finite sequence of well-defined instructions to solve a problem or perform a computation. And, an algorithmic technique is a general approach for implementing a process or computation. One of the most essential aspects of an algorithm is its performance. It assists in optimizing a process according to the available resources.\n",
        "\n",
        "\n",
        "Statistics is the discipline that concerns the collecting, organizing, analyzing, interpreting and presenting data. Statistical techniques are helpful in providing insights about data. For example, extreme values, mean, median, standard deviations are useful in exploring, summarizing, and visualizing data.\n",
        "\n",
        "Statistics is mainly branched into two categories\n",
        "\n",
        "\n",
        "Categories of Statistics\n",
        "Machine Learning is also referred as Applied Statistics or Statistical Learning. The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.\n",
        "\n",
        "What is Deep Learning?\n",
        "Deep learning is a machine learning technique that teaches computers to learn and improve by example. Deep learning is based on the concept of our brain cells called neurons that function to process and transmit information in the complex circuit, and eventually, our brain directs an action. Scientifically, taking this analogy to deep learning, neurons are termed as nodes; the complex circuits are called as Artificial Neural Network (ANN); action is the predicting outcome.\n",
        "\n",
        "Nodes\n",
        "A node or neuron is a computational unit that has one or more weighted input connections, a transfer function that combines the inputs logically, and an output connection. Nodes are then organized into layers to comprise a network, which is called as Artificial Neural Network.\n",
        "\n",
        "\n",
        "X1 and X2 are numerical inputs; w1 and w2 are weights; 1 input with weight b as Bias\n",
        "\n",
        "Artificial Neural Network\n",
        "Artificial neural network (ANN) consist of layers of nodes. Nodes within individual layers are connected to adjacent layers. The network is said to be deeper based on the number of layers it has. In an artificial neural network, data/information travels between nodes and assign corresponding weights. The final layer compiles the weighted inputs to produce an output.\n",
        "\n",
        "One of the main crux of ANN is Activation Function. Activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose is to introduce non-linearity into the output of a node as non-linear functions accept the complex nature of neural networks.\n",
        "\n",
        "\n",
        "Artificial Neural Network\n",
        "The ability to process large numbers of inputs (features) makes deep learning very powerful when dealing with unstructured data. However, deep learning algorithms can be overkill for less complex problems because they require access to a vast amount of data to be effective\n",
        "\n",
        "Outcome\n",
        "Each nodes in the individual layers generate an output that is proportional to node’s weight, which in turn becomes the input for the next layer, and so on. This happens repeatedly until the outcome is accomplished. The actual outcome is then compared with the expected outcome, and the network performance is evaluated. If the difference between actual and expected outcome is large then, an ANN technique called as backpropagation is implemented that adjusts the weight of the nodes to minimize this difference.\n",
        "\n",
        "What is Artificial Intelligence?\n",
        "Artificial intelligence (AI) is the ability of a machine or a computer program to think, act and learn like humans. AI is accomplished by studying how human brain thinks, acts and learns while trying to solve a problem, and then using the outcomes of this study, intelligent software and systems are developed. Some of the areas where AI has its prominence are depicted in the figure below\n",
        "\n",
        "\n",
        "Areas of AI\n",
        "Comparing Data Science (DS), Machine Learning (ML), Deep Learning (DL) and Artificial Intelligence (AI)\n",
        "After the brief introduction of each of these pillars, let us understand the commonalities them. AI ML DL are not part or subset of DS, although, certain tasks involved in DS intersect with AI, ML and DL. DS is a data-driven technique and each of DS, ML and DL have processes that relate to data or big data, contextually.\n",
        "\n",
        "\n",
        "Linking of DS, ML, DL, AI\n",
        "DS, ML, DL and AI need loads of data to begin with. Each of these processes the data in their own context and techniques and provide an outcome, which is then examined on human interests. A point to note is, DS, ML, DL and AI are iterative techniques that is, if the actual outcome has large variance with the expected result, the respective processes are repeated.\n",
        "\n",
        "\n",
        "Comparison overview — DS, ML, DL, AI\n",
        "If observed carefully, the processes contain the similarities between DS, ML, DL and AI. Linkage above two figures, following inferences can be made\n",
        "\n",
        "Point of reference: DS\n",
        "\n",
        "DS intersects with ML on the grounds of cleaning and modelling the data\n",
        "The cleaning and munging steps of DS help is feature extraction and grouping of information in DL\n",
        "The interference and decision making phases are advanced version of describing and modelling steps of DS\n",
        "Point of reference: ML\n",
        "\n",
        "Cleansing and preparing data in ML is the precursor step of extracting features in DL; also, the evaluation provided in ML acts as a prototype for evaluating the performance of neural network in DL\n",
        "Simply put, AI is applied based on ML or ML can be called as subset of AI.\n",
        "Point of reference: DL\n",
        "\n",
        "The evaluation and self-improving stages of DL assists in developing superior AI models."
      ],
      "metadata": {
        "id": "KVNbjWfitX0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. what are the main difference between supervised learning ,unsupervised learning and semi supervised learning .\n",
        "\n",
        "Supervised Learning\n",
        "Supervised learning models map inputs to outputs.\n",
        "\n",
        "Overview\n",
        "Supervised learning is typically done in the context of classification, when we want to map input to output labels, or regression, when we want to map input to a continuous output. Common algorithms in supervised learning include logistic regression, naive bayes, support vector machines, artificial neural networks, and random forests. In both regression and classification, the goal is to find specific relationships or structure in the input data that allow us to effectively produce correct output data.\n",
        "\n",
        "Note that “correct” output is determined entirely from the training data, so while we do have a ground truth that our model will assume is true, it is not to say that data labels are always correct in real-world situations. Noisy, or incorrect, data labels will clearly reduce the effectiveness of your model.\n",
        "\n",
        "Complexity\n",
        "Model complexity refers to the complexity of the function you are attempting to learn — similar to the degree of a polynomial. The proper level of model complexity is generally determined by the nature of your training data.\n",
        "\n",
        "If you have a small amount of data, or if your data is not uniformly spread throughout different possible scenarios, you should opt for a low-complexity model. This is because a high-complexity model will overfit if used on a small number of data points.\n",
        "\n",
        "Overfitting refers to learning a function that fits your training data very well, but does not generalize to other data points — in other words, you are strictly learning to produce your training data without learning the actual trend or structure in the data that leads to this output. Imagine trying to fit a curve between 2 points. In theory, you can use a function of any degree, but in practice, you would parsimoniously add complexity, and go with a linear function.\n",
        "\n",
        "Bias-variance trade-off\n",
        "The bias-variance trade-off also relates to model generalization. In any model, there is a balance between bias, which is the constant error term, and variance, which is the amount by which the error may vary between different data sets. So, high bias and low variance would be a model that is consistently wrong 20% of the time, whereas a low bias and high variance model would be a model that can be wrong anywhere from 5%-50% of the time, depending on the data used to train it.\n",
        "\n",
        "Note that bias and variance typically move in opposite directions of each other; increasing bias will usually lead to lower variance, and vice versa. When making your model, your specific problem and the nature of your data should allow you to make an informed decision on where to fall on the bias-variance spectrum.\n",
        "\n",
        "Generally, increasing bias (and decreasing variance) results in models with relatively guaranteed baseline levels of performance, which may be critical in certain tasks. Additionally, in order to produce models that generalize well, the variance of your model should scale with the size and complexity of your training data. Small, simple data-sets should usually be learned with low-variance models, and large, complex data-sets will often require higher-variance models to fully learn the structure of the data.\n",
        "\n",
        "Semi-supervised Learning\n",
        "Learning with both unlabaled and labeled data points.\n",
        "\n",
        "Overview\n",
        "Semi-supervised learning falls in between supervised and unsupervised learning. Semi-supervised models aim to use a small amount of labeled training data along with a large amount of unlabeled training data. This often occurs in real-world situations in which labeling data is very expensive, and/or you have a constant stream of data.\n",
        "\n",
        "For example, if we were trying to detect inappropriate messages in a social network, there is no way to obtain hand-labeled information on each message, as there are simply too many and it would be too costly. Instead, we can hand-label a subset of them, and leverage semi-supervised techniques to use this small set of labeled data to help us understand the rest of the messages’ content as they come in.\n",
        "\n",
        "Some common semi-supervised methods are transductive support vector machines, and graph-based methods such as label propagation.\n",
        "\n",
        "Assumptions\n",
        "Semi-supervised methods must make some assumption about the data in order to justify using a small set of labeled data to make conclusions about the unlabeled data points. These can be grouped into three categories.\n",
        "\n",
        "The first is the continuity assumption. This assumes that data points that are “close” to each other are more likely to have a common label.\n",
        "\n",
        "The second is the cluster assumption. This assumes that the data naturally forms discrete clusters, and that points in the same cluster are more likely to share a label.\n",
        "\n",
        "The third is the manifold assumption. This assumes that the data roughly lies in a lower-dimensional space (or manifold) than the input space. This scenario is relevant when an unobservable or difficult-to-observe system with a small number of parameters produces high-dimensional observable output.\n",
        "\n",
        "Unsupervised Learning\n",
        "Unsupervised models find inherent patterns in data.\n",
        "\n",
        "Overview\n",
        "The most common tasks within unsupervised learning are clustering, representation learning, and density estimation. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels. Some common algorithms include k-means clustering, principal component analysis, and autoencoders. Since no labels are provided, there is no specific way to compare model performance in most unsupervised learning methods.\n",
        "\n",
        "Exploratory data analysis (EDA)\n",
        "Unsupervised learning is very useful in exploratory analysis because it can automatically identify structure in data. For example, if an analyst were trying to segment consumers, unsupervised clustering methods would be a great starting point for their analysis. In situations where it is either impossible or impractical for a human to propose trends in the data, unsupervised learning can provide initial insights that can then be used to test individual hypotheses.\n",
        "\n",
        "Dimensionality reduction\n",
        "Dimensionality reduction, which refers to the methods used to represent data using less columns or features, can be accomplished through unsupervised methods. In representation learning, we wish to learn relationships between individual features, allowing us to represent our data using the latent features that interrelate our initial features. This sparse latent structure is often represented using far fewer features than we started with, so it can make further data processing much less intensive, and can eliminate redundant features. In other contexts, dimensionality reduction may be used to convert data from one modality to another. For example, a recurrent autoeconder may be used to convert sequences into a fixed length representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "xh3wuMyXvTKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. what is train ,test and validation split, explain the importance of each term\n",
        "\n",
        "\n",
        "Train-Valid-Test split is a technique to evaluate the performance of your machine learning model — classification or regression alike. You take a given dataset and divide it into three subsets. A brief description of the role of each of these datasets is below.\n",
        "\n",
        "Train Dataset\n",
        "\n",
        "Set of data used for learning (by the model), that is, to fit the parameters to the machine learning model\n",
        "\n",
        "Valid Dataset\n",
        "\n",
        "Set of data used to provide an unbiased evaluation of a model fitted on the training dataset while tuning model hyperparameters.\n",
        "Also play a role in other forms of model preparation, such as feature selection, threshold cut-off selection.\n",
        "\n",
        "Test Dataset\n",
        "\n",
        "Set of data used to provide an unbiased evaluation of a final model fitted on the training dataset.\n",
        "\n",
        "\n",
        "Whenever we train a machine learning model, we can’t train that model on a single dataset or even we train it on a single dataset then we will not be able to assess the performance of our model. For that reason, we split our source data into training, testing, and validation datasets. Now for understanding the need for data split let’s take an example of classroom teaching.\n",
        "\n",
        "Suppose a mathematics faculty teaches her students about an algorithm. For the explanation the teacher uses some examples, those examples are our training dataset. The student in this case is our machine learning model and the examples are part of the dataset. Because students are learning by those examples that’s why we call it our training set.\n",
        "\n",
        "And to check whether the students got the concepts of the algorithm correctly, the teacher give some practice problems to the students. By solving those problems students will evaluate their learning and if there is any difficulty they face, they will ask their doubt of the instructor(Feedback of model).\n",
        "\n",
        "There might be some misunderstanding between the students for some concepts because of which they were not able to solve the problem. So, the teacher might try to explain the problem to the students in a different way(Fine-tuning of parameters).  \n",
        "\n",
        "Students can also improve their learning by solving more and more practice problems(Validation). The more diverse the practice problems are great will be the learning(Cross-validation). Students can improve their accuracy(Cross-validation accuracy) by repeatedly solving practice problems.\n",
        "\n",
        "But once the class teaching is over and the exam comes, there is no going back to the teacher or solving practice problems. Whatever the students have learned, they need to use it for solving the problems given in the exam(Testing data). And the result that the student gets will be the final accuracy about how well that student learned about that concept.\n",
        "\n",
        "The summary of this analogy is:\n",
        "\n",
        "Training data = Classroom teaching\n",
        "\n",
        "Validation data = Practice problems\n",
        "\n",
        "Testing data = Exam questions"
      ],
      "metadata": {
        "id": "HUt2OHKRwKmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.how can unsupervised learning be used in anomaly detection\n",
        "\n",
        "The objective of Unsupervised Anomaly Detection is to detect previously unseen rare objects or events without any prior knowledge about these. The only information available is that the percentage of anomalies in the dataset is small, usually less than 1%. Since anomalies are rare and unknown to the user at training time, anomaly detection in most cases boils down to the problem of modelling the normal data distribution and defining a measurement in this space in order to classify samples as anomalous or normal. In high-dimensional data such as images, distances in the original space quickly lose descriptive power (curse of dimensionality) and a mapping to some more suitable space is required.\n",
        "\n",
        "Discovering a decision boundary for a one-class (normal) distribution (i.e., OCC training) is challenging in fully unsupervised settings as unlabeled training data include two classes (normal and abnormal). The challenge gets further exacerbated as the anomaly ratio gets higher for unlabeled data. To construct a robust OCC with unlabeled data, excluding likely-positive (anomalous) samples from the unlabeled data, the process referred to as data refinement, is critical. The refined data, with a lower anomaly ratio, are shown to yield superior anomaly detection models.\n",
        "\n",
        "SRR first refines data from an unlabeled dataset, then iteratively trains deep representations using refined data while improving the refinement of unlabeled data by excluding likely-positive samples. For data refinement, an ensemble of OCCs is employed, each of which is trained on a disjoint subset of unlabeled training data. If there is consensus among all the OCCs in the ensemble, the data that are predicted to be negative (normal) are included in the refined data. Finally, the refined training data are used to train the final OCC to generate the anomaly predictions."
      ],
      "metadata": {
        "id": "J_IyC5r7ykIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. List down some commonly used supervised learning algorithms and unsupervised learning algorithms \n",
        "\n",
        "supervised machine learning algorithms, such as:\n",
        "\n",
        "Decision Trees,\n",
        "Naive Bayes Classification,\n",
        "Support vector machines for classification problems,\n",
        "Random forest for classification and regression problems,\n",
        "Linear regression for regression problems,\n",
        "Ordinary Least Squares Regression,\n",
        "Logistic Regression,\n",
        "Ensemble Methods\n",
        "\n",
        "unsupervised learning algorithms are:\n",
        "\n",
        "K-means for clustering problems,\n",
        "Apriori algorithm for association rule learning problems,\n",
        "Principal Component Analysis,\n",
        "Singular Value Decomposition,\n",
        "Independent Component Analysis"
      ],
      "metadata": {
        "id": "u917vBsu0PKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuvnQo5he9E5"
      },
      "outputs": [],
      "source": []
    }
  ]
}