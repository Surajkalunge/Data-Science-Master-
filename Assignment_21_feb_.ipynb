{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
        "\n",
        "Web scraping is the process of using bots to extract content and data from a website.\n",
        "\n",
        "Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database. The scraper can then replicate entire website content elsewhere.\n",
        "\n",
        "Web scraping is used in a variety of digital businesses that rely on data harvesting. Legitimate use cases include:\n",
        "\n",
        "Search engine bots crawling a site, analyzing its content and then ranking it.\n",
        "Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites.\n",
        "Market research companies using scrapers to pull data from forums and social media (e.g., for sentiment analysis).\n",
        "\n",
        "1. Price Monitoring\n",
        "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
        "\n",
        "2. Market Research\n",
        "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
        "\n",
        "3. News Monitoring\n",
        "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
        "\n",
        "4. Sentiment Analysis\n",
        "If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.\n",
        "\n"
      ],
      "metadata": {
        "id": "_NK4FRhNVpoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the different methods used for Web Scraping?\n",
        "\n",
        "HTML Parsing\n",
        "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
        "\n",
        "DOM Parsing\n",
        "The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
        "\n",
        "Vertical Aggregation\n",
        "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
        "\n",
        "XPath\n",
        "XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
        "\n",
        "Google Sheets\n",
        "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check if a website can be scraped or is protected."
      ],
      "metadata": {
        "id": "hGn0Yz6VWh8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Beautiful Soup? Why is it used?\n",
        "\n",
        "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping. Three features make it powerful:\n",
        "\n",
        "Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree: a toolkit for dissecting a document and extracting what you need. It doesn't take much code to write an application\n",
        "Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8. You don't have to think about encodings, unless the document doesn't specify an encoding and Beautiful Soup can't detect one. Then you just have to specify the original encoding.\n",
        "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility."
      ],
      "metadata": {
        "id": "Lasek2xdW7_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why is flask used in this Web Scraping project?\n",
        "\n",
        "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
      ],
      "metadata": {
        "id": "AX48NoYmXacL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
        "\n",
        "elastic beanstalk\n",
        "\n",
        "A small manufacturing organization uses their expertise to expand their business by leaving their IT management to the AWS.\n",
        "A large enterprise spread across the globe can utilize the AWS to deliver the training to the distributed workforce.\n",
        "An architecture consulting company can use AWS to get the high-compute rendering of construction prototype.\n",
        "A media company can use the AWS to provide different types of content such as ebox or audio files to the worldwide files\n",
        "\n"
      ],
      "metadata": {
        "id": "xAMjRKuqbxl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDlHNiM7VmgC"
      },
      "outputs": [],
      "source": []
    }
  ]
}